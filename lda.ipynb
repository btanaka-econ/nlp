{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a6657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "       filename                                     cleaned_tokens\n",
      "0  k210121a.pdf  ['januari', 'monetari', 'polici', 'monetari', ...\n",
      "1  k210319a.pdf  ['march', 'effect', 'sustain', 'monetari', 'ea...\n",
      "2  k210427a.pdf  ['april', 'monetari', 'polici', 'monetari', 'p...\n",
      "3  k210618a.pdf  ['june', 'monetari', 'polici', 'monetari', 'po...\n",
      "4  k210716a.pdf  ['juli', 'monetari', 'polici', 'monetari', 'po...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Ensure NLTK data is downloaded (only needs to be run once)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('cleaned_english_corpus.csv')\n",
    "    # Assuming the CSV has columns like 'date' and 'text'\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_english_corpus.csv' not found. Please ensure the file is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ce19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (V): 3581\n",
      "Number of Documents: 39\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a single text document according to the paper's methodology.\n",
    "    \"\"\"\n",
    "    # 1. Convert to lower case\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # 3. Tokenize into words\n",
    "    tokens = text.split()\n",
    "    # 4. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "# Ensure your text column is named 'text' or change accordingly\n",
    "processed_docs = df['cleaned_tokens'].apply(preprocess_text)\n",
    "\n",
    "# Create the two main inputs for the LDA model\n",
    "# 1. Dictionary: A mapping of each unique token to an integer ID\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "# Optional: Filter out extreme values (e.g., words in < 5 docs or > 50% of docs)\n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# 2. Bag-of-Words (BoW) Corpus: Document representation as (token_id, count)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(f\"Vocabulary Size (V): {len(dictionary)}\")\n",
    "print(f\"Number of Documents: {len(bow_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3abd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA with K=10, alpha=5.0000, eta=0.0559\n"
     ]
    }
   ],
   "source": [
    "def train_lda_model(corpus, dictionary, num_topics):\n",
    "    \"\"\"\n",
    "    Trains a Gensim LDA model with specified hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): The Bag-of-Words corpus.\n",
    "        dictionary (corpora.Dictionary): The corpus dictionary.\n",
    "        num_topics (int): The number of topics (K).\n",
    "        \n",
    "    Returns:\n",
    "        gensim.models.LdaModel: The trained LDA model.\n",
    "    \"\"\"\n",
    "    # Calculate hyperparameters based on the paper's specification\n",
    "    V = len(dictionary)\n",
    "    alpha = 50 / num_topics\n",
    "    eta = 200 / V\n",
    "    \n",
    "    print(f\"Training LDA with K={num_topics}, alpha={alpha:.4f}, eta={eta:.4f}\")\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        passes=15,          # Number of passes through the corpus\n",
    "        iterations=400,     # Maximum number of iterations\n",
    "        random_state=42     # For reproducibility\n",
    "    )\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "# Example: Train a model with K=10 topics\n",
    "K = 10\n",
    "lda_model = train_lda_model(bow_corpus, dictionary, num_topics=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d640521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame of textual factors (X_t):\n",
      "    Topic_2   Topic_3   Topic_4   Topic_5   Topic_6   Topic_7   Topic_8  \\\n",
      "0  0.015211  0.020688  0.024482  0.012049  0.770797  0.009857  0.009769   \n",
      "1  0.006348  0.009155  0.503089  0.002517  0.027252  0.002455  0.002443   \n",
      "2  0.018578  0.019675  0.021411  0.011879  0.834146  0.011695  0.011582   \n",
      "3  0.017454  0.019884  0.013560  0.006455  0.250214  0.006327  0.006288   \n",
      "4  0.013608  0.030040  0.032961  0.010234  0.489727  0.008400  0.008344   \n",
      "\n",
      "    Topic_9  Topic_10  \n",
      "0  0.017413  0.009971  \n",
      "1  0.005590  0.002485  \n",
      "2  0.017802  0.011858  \n",
      "3  0.013811  0.006405  \n",
      "4  0.019305  0.008513  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_topic_probabilities(lda_model, corpus, num_topics):\n",
    "    \"\"\"\n",
    "    Extracts topic probabilities for each document and formats them into a DataFrame.\n",
    "    \"\"\"\n",
    "    # FIX 1: Initialize an empty list\n",
    "    topic_distributions = []\n",
    "    \n",
    "    for doc_bow in corpus:\n",
    "        # Get the topic distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
    "        \n",
    "        # Create a dense vector of probabilities, ensuring it matches the number of topics\n",
    "        topic_prob_dict = dict(doc_topics)\n",
    "        topic_prob_vector = [topic_prob_dict.get(i, 0.0) for i in range(num_topics)]\n",
    "        \n",
    "        topic_distributions.append(topic_prob_vector)\n",
    "        \n",
    "    # FIX 2: Create a list of column names for the DataFrame\n",
    "    column_names = [f'Topic_{i+1}' for i in range(num_topics)]\n",
    "    topic_df = pd.DataFrame(topic_distributions, columns=column_names)\n",
    "    \n",
    "    return topic_df\n",
    "\n",
    "# Extract the probabilities\n",
    "topic_prob_df = extract_topic_probabilities(lda_model, bow_corpus, num_topics=K)\n",
    "\n",
    "# Combine with original DataFrame (e.g., with dates)\n",
    "# Assuming df has a 'date' column and its length matches the corpus\n",
    "final_df = pd.concat([df[['filename']].reset_index(drop=True), topic_prob_df], axis=1)\n",
    "\n",
    "# FIX 3: Prepare the final matrix of K-1 exogenous variables for the VAR model\n",
    "# We drop the date and one topic (e.g., 'Topic_1') to use as a baseline\n",
    "X_t = final_df.drop(columns=['filename', 'Topic_1'])\n",
    "\n",
    "print(\"Final DataFrame of textual factors (X_t):\")\n",
    "print(X_t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd487e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words for each topic:\n",
      "Topic: 0 \n",
      "Words: 0.016*\"'purchas',\" + 0.012*\"'financi',\" + 0.012*\"'amount',\" + 0.010*\"'price',\" + 0.010*\"'yen',\" + 0.010*\"'polici',\" + 0.009*\"'covid',\" + 0.009*\"'jgb',\" + 0.009*\"'interest',\" + 0.008*\"'increas',\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.045*\"'price',\" + 0.015*\"'econom',\" + 0.014*\"'polici',\" + 0.013*\"'develop',\" + 0.013*\"'increas',\" + 0.013*\"'rise',\" + 0.012*\"'inflat',\" + 0.012*\"'cpi',\" + 0.011*\"'wage',\" + 0.011*\"'moder',\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.021*\"'monetari',\" + 0.020*\"'polici',\" + 0.018*\"'price',\" + 0.015*\"'interest',\" + 0.012*\"'effect',\" + 0.010*\"'review',\" + 0.009*\"'rate',\" + 0.009*\"'chart',\" + 0.009*\"'financi',\" + 0.009*\"'eas',\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.028*\"'interest',\" + 0.020*\"'financi',\" + 0.019*\"'yield',\" + 0.019*\"'rate',\" + 0.015*\"'price',\" + 0.014*\"'curv',\" + 0.013*\"'control',\" + 0.013*\"'effect',\" + 0.012*\"'monetari',\" + 0.012*\"'purchas',\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.010*\"'monetari',\" + 0.010*\"'polici',\" + 0.010*\"'price',\" + 0.007*\"'increas',\" + 0.006*\"'financi',\" + 0.005*\"'purchas',\" + 0.004*\"'interest',\" + 0.004*\"regard,\" + 0.004*\"come\" + 0.004*\"affect\"\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.015*\"'price',\" + 0.013*\"'purchas',\" + 0.013*\"'polici',\" + 0.011*\"'increas',\" + 0.010*\"polici\" + 0.009*\"'monetari',\" + 0.009*\"'financ',\" + 0.009*\"'yen',\" + 0.009*\"'continu',\" + 0.008*\"'target',\"\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.015*\"'polici',\" + 0.012*\"'monetari',\" + 0.008*\"'price',\" + 0.005*\"'increas',\" + 0.005*\"'interest',\" + 0.005*\"'financi',\" + 0.004*\"'purchas',\" + 0.003*\"'eas',\" + 0.003*\"'effect',\" + 0.003*\"'econom',\"\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.010*\"'monetari',\" + 0.009*\"'price',\" + 0.009*\"'polici',\" + 0.005*\"'financi',\" + 0.004*\"'interest',\" + 0.004*\"'purchas',\" + 0.004*\"'econom',\" + 0.004*\"'increas',\" + 0.003*\"'eas',\" + 0.002*\"'effect',\"\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.022*\"'price',\" + 0.021*\"'polici',\" + 0.020*\"'monetari',\" + 0.012*\"'effect',\" + 0.011*\"'review',\" + 0.010*\"'interest',\" + 0.010*\"'eas',\" + 0.009*\"'financi',\" + 0.008*\"'firm',\" + 0.008*\"'expect',\"\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.015*\"'polici',\" + 0.014*\"'monetari',\" + 0.011*\"'price',\" + 0.006*\"'interest',\" + 0.004*\"'eas',\" + 0.004*\"'financi',\" + 0.004*\"'purchas',\" + 0.004*\"'effect',\" + 0.003*\"'increas',\" + 0.003*\"'firm',\"\n",
      "\n",
      "\n",
      "Coherence Score (C_v): 0.3518\n"
     ]
    }
   ],
   "source": [
    "# 1. Print top words for each topic\n",
    "print(\"\\nTop words for each topic:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "\n",
    "# 2. Calculate topic coherence\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'\\nCoherence Score (C_v): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78994f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulated Macroeconomic Data (Endogenous) ---\n",
      "            inflation  gdp_growth  interest_rate\n",
      "2010-01-01   0.496714   -0.138264       0.647689\n",
      "2010-02-01   2.019744   -0.372418       0.413552\n",
      "2010-03-01   3.598957    0.395017      -0.055923\n",
      "2010-04-01   4.141517   -0.068401      -0.521653\n",
      "2010-05-01   4.383479   -1.981681      -2.246570\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (150, 9), indices imply (150, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# The textual factors are the K-1 topic probabilities.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m exog_cols \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 38\u001b[0m textual_factors_Xt \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Simulated Textual Factors (Exogenous) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(textual_factors_Xt\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\tnk20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\tnk20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tnk20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (150, 9), indices imply (150, 0)"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  VAR and VAR-teXt Implementation for Macroeconomic Forecasting\n",
    "# =============================================================================\n",
    "#\n",
    "#  Description: This script implements a baseline Vector Autoregression (VAR)\n",
    "#               model and a text-augmented VAR (VAR-teXt) using the 'statsmodels'\n",
    "#               library. It is designed to integrate the textual factors (X_t)\n",
    "#               generated from an LDA model as exogenous variables.\n",
    "#\n",
    "#  Required Libraries:\n",
    "#  pip install pandas numpy statsmodels matplotlib\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Data Preparation ---\n",
    "# This section simulates macroeconomic data and the textual factors (X_t).\n",
    "#!!! IMPORTANT: REPLACE THIS SECTION WITH YOUR ACTUAL DATA LOADING!!!\n",
    "\n",
    "# Simulate macroeconomic data (endogenous variables)\n",
    "np.random.seed(42)\n",
    "dates = pd.to_datetime(pd.date_range(start='2010-01-01', periods=150, freq='MS'))\n",
    "macro_data = pd.DataFrame(np.random.randn(150, 3), columns=['inflation', 'gdp_growth', 'interest_rate'], index=dates)\n",
    "macro_data = macro_data.cumsum() # Create some trend to make it more realistic\n",
    "print(\"--- Simulated Macroeconomic Data (Endogenous) ---\")\n",
    "print(macro_data.head())\n",
    "\n",
    "# Simulate the textual factors DataFrame (X_t)\n",
    "# This should have the same index as your macro data.\n",
    "num_topics = 10\n",
    "# The textual factors are the K-1 topic probabilities.\n",
    "exog_cols = []\n",
    "textual_factors_Xt = pd.DataFrame(np.random.rand(150, num_topics - 1), columns=exog_cols, index=dates)\n",
    "print(\"\\n--- Simulated Textual Factors (Exogenous) ---\")\n",
    "print(textual_factors_Xt.head())\n",
    "\n",
    "# --- 2. Pre-Modeling Analysis: Stationarity ---\n",
    "# VAR models assume the time series are stationary. A common way to achieve this\n",
    "# is by differencing the data. We use the Augmented Dickey-Fuller test to check.\n",
    "\n",
    "def check_stationarity(data):\n",
    "    \"\"\"Performs ADF test and prints results.\"\"\"\n",
    "    for col in data.columns:\n",
    "        result = adfuller(data[col])\n",
    "        print(f'ADF Test for {col}:')\n",
    "        print(f'  ADF Statistic: {result}')\n",
    "        print(f'  p-value: {result[1]}')\n",
    "        if result[1] > 0.05:\n",
    "            print(f'  Result: {col} is likely non-stationary (p > 0.05)\\n')\n",
    "        else:\n",
    "            print(f'  Result: {col} is likely stationary (p <= 0.05)\\n')\n",
    "\n",
    "print(\"\\n--- Stationarity Check on Original Data ---\")\n",
    "check_stationarity(macro_data)\n",
    "\n",
    "# If data is non-stationary, apply differencing\n",
    "macro_data_stationary = macro_data.diff().dropna()\n",
    "print(\"\\n--- Stationarity Check on Differenced Data ---\")\n",
    "check_stationarity(macro_data_stationary)\n",
    "\n",
    "\n",
    "# --- 3. Align Data and Split into Training/Testing Sets ---\n",
    "\n",
    "# Align the exogenous (text) data with the stationary endogenous (macro) data.\n",
    "# The VAR-teXt model uses lagged textual factors (X_{t-1}) to predict Y_t.\n",
    "# We shift the textual factors by one period.\n",
    "textual_factors_lagged = textual_factors_Xt.shift(1)\n",
    "\n",
    "# Combine and drop any rows with NaN values resulting from differencing and shifting\n",
    "combined_data = pd.concat([macro_data_stationary, textual_factors_lagged], axis=1).dropna()\n",
    "\n",
    "# Separate back into endogenous and exogenous DataFrames\n",
    "endog_data = combined_data[['inflation', 'gdp_growth', 'interest_rate']]\n",
    "exog_data = combined_data[exog_cols]\n",
    "\n",
    "# Define the test set size (e.g., last 24 months for forecasting)\n",
    "n_forecast = 24\n",
    "endog_train = endog_data.iloc[:-n_forecast]\n",
    "endog_test = endog_data.iloc[-n_forecast:]\n",
    "exog_train = exog_data.iloc[:-n_forecast]\n",
    "exog_test = exog_data.iloc[-n_forecast:]\n",
    "\n",
    "print(f\"\\nTraining data shape (Endogenous): {endog_train.shape}\")\n",
    "print(f\"Test data shape (Endogenous):     {endog_test.shape}\")\n",
    "print(f\"Training data shape (Exogenous): {exog_train.shape}\")\n",
    "print(f\"Test data shape (Exogenous):     {exog_test.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Baseline VAR Model (without textual factors) ---\n",
    "\n",
    "print(\"\\n--- Fitting Baseline VAR Model ---\")\n",
    "# Instantiate the VAR model on the endogenous training data\n",
    "model_baseline = VAR(endog_train)\n",
    "\n",
    "# Select the optimal lag order (p) using AIC\n",
    "# This is a crucial step for VAR model specification\n",
    "lag_selection_results = model_baseline.select_order(maxlags=10)\n",
    "selected_lags = lag_selection_results.aic\n",
    "print(f\"Optimal lag order selected by AIC: {selected_lags}\")\n",
    "\n",
    "# Fit the model with the selected lag order\n",
    "results_baseline = model_baseline.fit(selected_lags)\n",
    "print(\"\\nBaseline VAR Model Summary:\")\n",
    "print(results_baseline.summary())\n",
    "\n",
    "# Generate forecasts\n",
    "# The number of initial observations must match the lag order\n",
    "lag_order_baseline = results_baseline.k_ar\n",
    "forecast_input_baseline = endog_data.values[-n_forecast - lag_order_baseline:-n_forecast]\n",
    "forecast_baseline = results_baseline.forecast(y=forecast_input_baseline, steps=n_forecast)\n",
    "\n",
    "# Create a DataFrame for the forecasts\n",
    "forecast_df_baseline = pd.DataFrame(forecast_baseline, index=endog_test.index, columns=[f'{col}_pred' for col in endog_test.columns])\n",
    "\n",
    "\n",
    "# --- 5. VAR-teXt Model (with textual factors) ---\n",
    "\n",
    "print(\"\\n--- Fitting VAR-teXt Model ---\")\n",
    "# Instantiate the VAR model with both endogenous and exogenous training data\n",
    "model_text = VAR(endog=endog_train, exog=exog_train)\n",
    "\n",
    "# Fit the model using the same lag order for comparability\n",
    "results_text = model_text.fit(selected_lags)\n",
    "print(\"\\nVAR-teXt Model Summary:\")\n",
    "print(results_text.summary())\n",
    "\n",
    "# Generate forecasts\n",
    "# The forecast method requires future values of the exogenous variables\n",
    "lag_order_text = results_text.k_ar\n",
    "forecast_input_text = endog_data.values[-n_forecast - lag_order_text:-n_forecast]\n",
    "forecast_text = results_text.forecast(y=forecast_input_text, steps=n_forecast, exog_future=exog_test.values)\n",
    "\n",
    "# Create a DataFrame for the forecasts\n",
    "forecast_df_text = pd.DataFrame(forecast_text, index=endog_test.index, columns=[f'{col}_pred' for col in endog_test.columns])\n",
    "\n",
    "\n",
    "# --- 6. Evaluate and Visualize Forecasts ---\n",
    "\n",
    "print(\"\\n--- Forecast Evaluation ---\")\n",
    "# Combine actuals and forecasts for comparison\n",
    "results_df_baseline = pd.concat([endog_test, forecast_df_baseline], axis=1)\n",
    "results_df_text = pd.concat([endog_test, forecast_df_text], axis=1)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for each model\n",
    "rmse_baseline = np.sqrt(np.mean((results_df_baseline['inflation'] - results_df_baseline['inflation_pred'])**2))\n",
    "rmse_text = np.sqrt(np.mean((results_df_text['inflation'] - results_df_text['inflation_pred'])**2))\n",
    "\n",
    "print(f\"RMSE for Baseline VAR (Inflation): {rmse_baseline:.4f}\")\n",
    "print(f\"RMSE for VAR-teXt (Inflation):     {rmse_text:.4f}\")\n",
    "\n",
    "if rmse_text < rmse_baseline:\n",
    "    print(\"\\nThe VAR-teXt model shows improved forecasting performance based on RMSE.\")\n",
    "else:\n",
    "    print(\"\\nThe VAR-teXt model did not improve forecasting performance based on RMSE.\")\n",
    "\n",
    "# Plot the results for one variable (e.g., inflation)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(endog_data.index, endog_data['inflation'], label='Actual Inflation', color='black')\n",
    "plt.plot(results_df_baseline.index, results_df_baseline['inflation_pred'], label='Baseline VAR Forecast', linestyle='--', color='blue')\n",
    "plt.plot(results_df_text.index, results_df_text['inflation_pred'], label='VAR-teXt Forecast', linestyle='--', color='red')\n",
    "plt.title('Inflation Forecast: Baseline VAR vs. VAR-teXt')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Inflation (Differenced)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
