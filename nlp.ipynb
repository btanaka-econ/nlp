{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d821aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting English Text Pre-processing Pipeline ---\n",
      "Successfully extracted text from 39 PDF documents.\n",
      "Processing all documents (Cleaning, Tokenizing, Stemming)...\n",
      "--- Pipeline Complete. Cleaned corpus saved to cleaned_english_corpus.csv ---\n",
      "\n",
      "Sample of cleaned tokens from the first document:\n",
      "['januari', 'monetari', 'polici', 'monetari', 'polici', 'meet', 'held', 'today', 'polici', 'board', 'decid', 'upon', 'follow', 'yield', 'curv', 'control', 'decid', 'major', 'vote', 'set']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#  Python Script for Pre-processing English Central Bank Texts\n",
    "# =============================================================================\n",
    "\n",
    "# --- Environment Setup and Library Imports ---\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Dict, Set\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "nltk.download('names')\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "# --- Download NLTK Data (Robust Method) ---\n",
    "# This ensures all required packages are present before the script runs.\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt' package...\")\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'stopwords' package...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# --- 1. Configuration and Constants ---\n",
    "\n",
    "DOMAIN_SPECIFIC_STOPWORDS: Set[str] = {\n",
    "    'bank', 'japan', 'committee', 'policy', 'rate', 'fomc', 'federal',\n",
    "    'meeting', 'statement', 'economic', 'financial', 'market'\n",
    "}\n",
    "MIN_TOKEN_LENGTH: int = 3\n",
    "\n",
    "# --- 2. Core Processing Functions ---\n",
    "\n",
    "def extract_text_from_pdfs(pdf_directory: str) -> pd.DataFrame:\n",
    "    # (This function remains the same)\n",
    "    pdf_path = Path(pdf_directory)\n",
    "    if not pdf_path.is_dir():\n",
    "        raise FileNotFoundError(f\"Directory does not exist: {pdf_directory}\")\n",
    "    documents: List[Dict[str, str]] = []\n",
    "    for pdf_file in sorted(pdf_path.glob(\"*.pdf\")):\n",
    "        try:\n",
    "            with fitz.open(pdf_file) as doc:\n",
    "                raw_text = \"\".join(page.get_text(\"text\") for page in doc)\n",
    "                if raw_text.strip():\n",
    "                    documents.append({\"filename\": pdf_file.name, \"raw_text\": raw_text})\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process file {pdf_file.name}: {e}\")\n",
    "    if not documents:\n",
    "        print(f\"Warning: No text extracted from PDFs in {pdf_directory}.\")\n",
    "        return pd.DataFrame(columns=[\"filename\", \"raw_text\"])\n",
    "    df = pd.DataFrame(documents)\n",
    "    print(f\"Successfully extracted text from {len(df)} PDF documents.\")\n",
    "    return df\n",
    "\n",
    "def create_english_preprocessor():\n",
    "    # (This function remains the same)\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmed_domain_stopwords = {stemmer.stem(word) for word in DOMAIN_SPECIFIC_STOPWORDS}\n",
    "    stop_words.update(stemmed_domain_stopwords)\n",
    "    def preprocess_document(text: str) -> List[str]:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        tokens = word_tokenize(text)\n",
    "        processed_tokens = [\n",
    "            stemmer.stem(token) for token in tokens \n",
    "            if token not in stop_words and len(token) >= MIN_TOKEN_LENGTH\n",
    "        ]\n",
    "        return processed_tokens\n",
    "    return preprocess_document\n",
    "\n",
    "# --- 3. Main Execution Pipeline ---\n",
    "\n",
    "def main_pipeline(pdf_directory: str, output_csv_path: str = \"cleaned_english_corpus.csv\"):\n",
    "    print(\"--- Starting English Text Pre-processing Pipeline ---\")\n",
    "\n",
    "    # Step 1: Extract text from PDFs\n",
    "    df = extract_text_from_pdfs(pdf_directory)\n",
    "    if df.empty:\n",
    "        print(\"Pipeline halted as no data was extracted.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Create the preprocessor function\n",
    "    english_preprocessor = create_english_preprocessor()\n",
    "\n",
    "    # Step 3: Apply the preprocessing to all documents\n",
    "    print(\"Processing all documents (Cleaning, Tokenizing, Stemming)...\")\n",
    "    \n",
    "    # --- MODIFICATION START ---\n",
    "    # This block will catch the error and print the full message\n",
    "    try:\n",
    "        df['cleaned_tokens'] = df['raw_text'].apply(english_preprocessor)\n",
    "    except LookupError as e:\n",
    "        print(\"\\n--- NLTK Error Caught ---\")\n",
    "        print(\"A specific NLTK resource is still missing.\")\n",
    "        print(f\"FULL ERROR MESSAGE: {e}\")\n",
    "        print(\"---------------------------\\n\")\n",
    "        # Stop the pipeline after getting the error\n",
    "        return\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    # Step 4: Save results\n",
    "    final_df = df[['filename', 'cleaned_tokens']]\n",
    "    final_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"--- Pipeline Complete. Cleaned corpus saved to {output_csv_path} ---\")\n",
    "\n",
    "    # Display a sample of the output\n",
    "    print(\"\\nSample of cleaned tokens from the first document:\")\n",
    "    if not final_df.empty and final_df.iloc[0]['cleaned_tokens']:\n",
    "        print(final_df.iloc[0]['cleaned_tokens'][:20])\n",
    "    else:\n",
    "        print(\"No tokens were generated for the first document.\")\n",
    "# --- 4. Script Execution ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # (This section remains the same)\n",
    "    PDF_SOURCE_DIRECTORY = \"boj_statements_english\"\n",
    "    Path(PDF_SOURCE_DIRECTORY).mkdir(exist_ok=True)\n",
    "    if not any(Path(PDF_SOURCE_DIRECTORY).iterdir()):\n",
    "        print(f\"Error: The directory '{PDF_SOURCE_DIRECTORY}' is empty.\")\n",
    "        print(\"Please add your Bank of Japan PDF files to this directory and run the script again.\")\n",
    "    else:\n",
    "        main_pipeline(pdf_directory=PDF_SOURCE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d09511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tnk20\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
